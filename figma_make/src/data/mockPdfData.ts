import { PdfFile } from '../App';

export const mockPdfData: PdfFile[] = [
  {
    id: '1',
    title: 'Attention Is All You Need: A Comprehensive Study of Transformer Architectures',
    authors: ['Ashish Vaswani', 'Noam Shazeer', 'Niki Parmar', 'Jakob Uszkoreit'],
    year: 2025,
    conference: 'NeurIPS',
    subject: ['Deep Learning', 'NLP', 'Transformers'],
    abstract: 'We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train.',
    pdfUrl: '/papers/attention.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1677442136019-21780ecad995?w=400&h=500&fit=crop',
    citations: 15234,
    doi: '10.48550/arXiv.1706.03762',
    hasBlogView: true
  },
  {
    id: '2',
    title: 'Deep Residual Learning for Image Recognition',
    authors: ['Kaiming He', 'Xiangyu Zhang', 'Shaoqing Ren', 'Jian Sun'],
    year: 2024,
    conference: 'CVPR',
    subject: ['Computer Vision', 'Deep Learning', 'Image Recognition'],
    abstract: 'Deep residual networks have revolutionized computer vision by enabling training of very deep neural networks. We present residual learning framework to ease the training of networks that are substantially deeper than those used previously.',
    pdfUrl: '/papers/resnet.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1712847331632-c28a1a0dc238?w=400&h=500&fit=crop',
    citations: 18567,
    doi: '10.1109/CVPR.2016.90',
    hasBlogView: false
  },
  {
    id: '3',
    title: 'BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding',
    authors: ['Jacob Devlin', 'Ming-Wei Chang', 'Kenton Lee', 'Kristina Toutanova'],
    year: 2025,
    conference: 'ACL',
    subject: ['NLP', 'Deep Learning', 'Language Models'],
    abstract: 'We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models, BERT is designed to pre-train deep bidirectional representations from unlabeled text.',
    pdfUrl: '/papers/bert.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1655635643568-f30be9e69f58?w=400&h=500&fit=crop',
    citations: 12890,
    doi: '10.18653/v1/N19-1423',
    hasBlogView: true
  },
  {
    id: '4',
    title: 'Generative Adversarial Networks: A Survey and Taxonomy',
    authors: ['Ian Goodfellow', 'Jean Pouget-Abadie', 'Mehdi Mirza', 'Bing Xu'],
    year: 2024,
    conference: 'ICML',
    subject: ['Deep Learning', 'Generative Models', 'GANs'],
    abstract: 'Generative adversarial networks (GANs) have been a revolutionary force in machine learning, enabling the generation of realistic synthetic data. This survey provides a comprehensive overview of GAN architectures, training techniques, and applications.',
    pdfUrl: '/papers/gans.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1620712943543-bcc4688e7485?w=400&h=500&fit=crop',
    citations: 9456,
    doi: '10.1145/3301282',
    hasBlogView: false
  },
  {
    id: '5',
    title: 'Neural Architecture Search: A Survey',
    authors: ['Thomas Elsken', 'Jan Hendrik Metzen', 'Frank Hutter'],
    year: 2025,
    conference: 'ICLR',
    subject: ['Deep Learning', 'AutoML', 'Neural Architecture Search'],
    abstract: 'Neural Architecture Search (NAS) aims to automate the design of neural network architectures. This comprehensive survey covers the major approaches to NAS, including reinforcement learning, evolutionary algorithms, and gradient-based methods.',
    pdfUrl: '/papers/nas.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1639762681485-074b7f938ba0?w=400&h=500&fit=crop',
    citations: 5678,
    doi: '10.1007/s10994-019-05823-6',
    hasBlogView: true
  },
  {
    id: '6',
    title: 'Reinforcement Learning: An Introduction to Deep Q-Networks',
    authors: ['Volodymyr Mnih', 'Koray Kavukcuoglu', 'David Silver'],
    year: 2024,
    conference: 'AAAI',
    subject: ['Reinforcement Learning', 'Deep Learning', 'Game AI'],
    abstract: 'We present deep Q-networks (DQN), a model-free reinforcement learning algorithm that combines Q-learning with deep neural networks. DQN is capable of learning successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning.',
    pdfUrl: '/papers/dqn.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1677756119517-756a188d2d94?w=400&h=500&fit=crop',
    citations: 8234,
    doi: '10.1038/nature14236',
    hasBlogView: false
  },
  {
    id: '7',
    title: 'Graph Neural Networks: A Review of Methods and Applications',
    authors: ['Jie Zhou', 'Ganqu Cui', 'Shengding Hu', 'Zhengyan Zhang'],
    year: 2025,
    conference: 'KDD',
    subject: ['Graph Neural Networks', 'Deep Learning', 'Graph Learning'],
    abstract: 'Graph neural networks (GNNs) have emerged as a powerful tool for learning on graph-structured data. This review provides a comprehensive overview of GNN architectures, training methods, and diverse applications across domains.',
    pdfUrl: '/papers/gnn.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1639322537504-6427a16b0a28?w=400&h=500&fit=crop',
    citations: 6789,
    doi: '10.1609/aaai.v34i04.5833',
    hasBlogView: true
  },
  {
    id: '8',
    title: 'Self-Supervised Learning: Generative or Contrastive',
    authors: ['Xiao Liu', 'Fanjin Zhang', 'Zhenyu Hou', 'Li Mian'],
    year: 2024,
    conference: 'NeurIPS',
    subject: ['Self-Supervised Learning', 'Deep Learning', 'Representation Learning'],
    abstract: 'Self-supervised learning has become a cornerstone of modern machine learning, enabling models to learn useful representations from unlabeled data. This paper provides a comprehensive comparison of generative and contrastive approaches to self-supervised learning.',
    pdfUrl: '/papers/ssl.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1676911809746-7ea10e1f6e79?w=400&h=500&fit=crop',
    citations: 4567,
    doi: '10.1109/TPAMI.2021.3060551',
    hasBlogView: false
  },
  {
    id: '9',
    title: 'Vision Transformers for Image Classification: A Comparative Study',
    authors: ['Alexey Dosovitskiy', 'Lucas Beyer', 'Alexander Kolesnikov'],
    year: 2025,
    conference: 'ICCV',
    subject: ['Computer Vision', 'Transformers', 'Image Classification'],
    abstract: 'While the Transformer architecture has become the standard for natural language processing tasks, its applications to computer vision remain limited. We show that a pure transformer applied directly to sequences of image patches can perform very well on image classification tasks.',
    pdfUrl: '/papers/vit.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1664447972779-316251bd8bd7?w=400&h=500&fit=crop',
    citations: 7890,
    doi: '10.48550/arXiv.2010.11929',
    hasBlogView: true
  },
  {
    id: '10',
    title: 'Few-Shot Learning: A Survey and New Perspectives',
    authors: ['Yaqing Wang', 'Quanming Yao', 'James Kwok', 'Lionel M. Ni'],
    year: 2024,
    conference: 'IJCAI',
    subject: ['Few-Shot Learning', 'Meta-Learning', 'Transfer Learning'],
    abstract: 'Few-shot learning aims to learn a classifier that can recognize new classes with only a few labeled examples. This survey provides a comprehensive overview of few-shot learning methods, including metric learning, meta-learning, and data augmentation approaches.',
    pdfUrl: '/papers/fsl.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1655720031554-a929595ffad7?w=400&h=500&fit=crop',
    citations: 3456,
    doi: '10.1145/3386252',
    hasBlogView: false
  },
  {
    id: '11',
    title: 'Explainable AI: A Review of Machine Learning Interpretability Methods',
    authors: ['Christoph Molnar', 'Giuseppe Casalicchio', 'Bernd Bischl'],
    year: 2025,
    conference: 'ICML',
    subject: ['Explainable AI', 'Interpretability', 'Machine Learning'],
    abstract: 'As machine learning models become increasingly complex, understanding their decision-making processes becomes crucial. This comprehensive review examines methods for interpreting and explaining machine learning models, from simple linear models to deep neural networks.',
    pdfUrl: '/papers/xai.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1676573409509-307d0b3f59c6?w=400&h=500&fit=crop',
    citations: 5234,
    doi: '10.1007/s10994-020-05901-0',
    hasBlogView: true
  },
  {
    id: '12',
    title: 'Federated Learning: Challenges, Methods, and Future Directions',
    authors: ['Qiang Yang', 'Yang Liu', 'Tianjian Chen', 'Yongxin Tong'],
    year: 2024,
    conference: 'ACL',
    subject: ['Federated Learning', 'Privacy', 'Distributed Systems'],
    abstract: 'Federated learning enables training machine learning models on distributed datasets without centralizing the data. This paper surveys the current state of federated learning, discussing challenges related to privacy, communication efficiency, and model personalization.',
    pdfUrl: '/papers/fl.pdf',
    thumbnailUrl: 'https://images.unsplash.com/photo-1666112835156-45d4969916be?w=400&h=500&fit=crop',
    citations: 6123,
    doi: '10.1145/3373376.3378521',
    hasBlogView: false
  }
];